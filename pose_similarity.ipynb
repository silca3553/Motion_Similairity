{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 node를 넣으면 벡터로 변환하여 리턴하는 함수\n",
    "def makevector(pointA,pointB):\n",
    "        #calculate vector between the two points (큰 노드에서 작은 노드로)\n",
    "        vector = pointB-pointA\n",
    "        return vector\n",
    "\n",
    "# 두 vector data들의 cos 값 평균을 구하는 함수\n",
    "def cos_sum(vectordata1, vectordata2):\n",
    "    ls = [] #초기화\n",
    "    for i in range(13):\n",
    "        x = vectordata1[i]\n",
    "        y = vectordata2[i]\n",
    "        ls.append(check_degree(x,y))\n",
    "        #print(check_degree(x,y))\n",
    "        #cos = round(np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),5)\n",
    "        #ls.append( 1- (0.25*((cos+1)**2)) )  #0~1\n",
    "              \n",
    "    return sum(ls)/13\n",
    "\n",
    "#두 벡터의 각도 차\n",
    "def check_degree(i,j):\n",
    "    x = np.dot(i,j ) / (np.linalg.norm(i) * np.linalg.norm(j))\n",
    "    if x > 90:\n",
    "        x = 90\n",
    "    return math.degrees( math.acos( round(x,5) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each frame of the video \n",
    "# 영상의 각 frame별 landmark들을 추출하여 리스트에 저장 (data1,data2)\n",
    "\n",
    "def get_framedata(video_name): \n",
    "\n",
    "    # Load MediaPipe pose model\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "    \n",
    "    # Initialize the pose model\n",
    "    pose1 = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    # Open the video file\n",
    "    sample1 = cv2.VideoCapture(video_name)\n",
    "\n",
    "    framedata1 = [] #total video1 vector data by frame\n",
    "\n",
    "    print(\"extracting vector...   video:\",video_name)\n",
    "    \n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret1, frame1 = sample1.read()\n",
    "        \n",
    "        if not ret1:\n",
    "            print(\"complete!\")\n",
    "            break\n",
    "        \n",
    "        # Convert the frame from BGR to RGB\n",
    "        frame1.flags.writeable = False\n",
    "        image1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)  \n",
    "        # Run the pose model on the frame\n",
    "        results1 = pose1.process(image1)\n",
    "        \n",
    "        if not results1.pose_landmarks:\n",
    "            continue\n",
    "            \n",
    "        landmark1 = []\n",
    "        \n",
    "        for i in range(33):\n",
    "            if i>0 and i<11: # 1,2,3,4,5,6,7,8,9,10 점 제외\n",
    "                continue\n",
    "            elif i>16 and i<23: # 17,18,19,20,21,22 점 제외\n",
    "                continue\n",
    "            elif i>28 and i<33: # 29,30,31,32 점 제외\n",
    "                continue\n",
    "            \n",
    "            marki = results1.pose_landmarks.landmark[i]\n",
    "            landmark1 += [(marki.x,marki.y,marki.z)]\n",
    "            \n",
    "        landmark1 = np.array(landmark1)\n",
    "\n",
    "        #13개의 vector를 저장하는 lists\n",
    "        vectordata1 = [] # video 1의 것\n",
    "\n",
    "        #landmark1에 들어있는 점들로 벡터 만들어서 vectordata에 저장 (13개)\n",
    "        vectordata1.append( makevector((landmark1[1] + landmark1[2])/2, landmark1[0]) )\n",
    "        vectordata1.append( makevector(landmark1[5], landmark1[3]) )\n",
    "        vectordata1.append( makevector(landmark1[3], landmark1[1]) )\n",
    "        vectordata1.append( makevector(landmark1[2], landmark1[1]) )\n",
    "        vectordata1.append( makevector(landmark1[4], landmark1[2]) )\n",
    "        vectordata1.append( makevector(landmark1[6], landmark1[4]) )\n",
    "        vectordata1.append( makevector(landmark1[8], landmark1[2]) )\n",
    "        vectordata1.append( makevector(landmark1[7], landmark1[1]) )\n",
    "        vectordata1.append( makevector(landmark1[8], landmark1[7]) )\n",
    "        vectordata1.append( makevector(landmark1[10], landmark1[8]) )\n",
    "        vectordata1.append( makevector(landmark1[12], landmark1[10]) )\n",
    "        vectordata1.append( makevector(landmark1[11], landmark1[9]) )\n",
    "        vectordata1.append( makevector(landmark1[9], landmark1[7]) )\n",
    "        \n",
    "        framedata1+= [vectordata1]\n",
    "\n",
    "        # Draw the pose skeleton on the frame\n",
    "        # mp_drawing.draw_landmarks(frame1, results1.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "        #                           mp_drawing.DrawingSpec(color=(255, 0, 0)),\n",
    "        #                          mp_drawing.DrawingSpec(color=(0, 255, 0)))\n",
    "        \n",
    "        # mp_drawing.draw_landmarks(frame2, results2.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "        #                           mp_drawing.DrawingSpec(color=(255, 0, 0)),\n",
    "        #                           mp_drawing.DrawingSpec(color=(0, 255, 0)))\n",
    "        # Resize the frame\n",
    "        # resized_frame1 = cv2.resize(frame1,(width1//4,height1//4))\n",
    "        # resized_frame2 = cv2.resize(frame2,(width2//4,height2//4))\n",
    "\n",
    "        # Show the frame\n",
    "        # cv2.imshow(\"Video1\", resized_frame1)\n",
    "        # cv2.imshow(\"Video2\", resized_frame2)\n",
    "        \n",
    "        # Wait for the user to press 'q' to exit\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "        \n",
    "    # Release the video file and close the window\n",
    "    sample1.release()\n",
    "    return framedata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting vector...   video: sample1.mp4\n",
      "complete!\n",
      "extracting vector...   video: sample3.mp4\n",
      "complete!\n",
      "start DTW..\n",
      "similarity: 56.859 %\n",
      "344\n",
      "[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (1, 32), (1, 33), (1, 34), (1, 35), (1, 36), (1, 37), (1, 38), (1, 39), (1, 40), (1, 41), (1, 42), (1, 43), (1, 44), (1, 45), (1, 46), (1, 47), (1, 48), (1, 49), (1, 50), (2, 51), (2, 52), (2, 53), (2, 54), (2, 55), (2, 56), (2, 57), (2, 58), (2, 59), (3, 60), (4, 61), (5, 62), (6, 63), (7, 64), (8, 65), (9, 66), (9, 67), (9, 68), (9, 69), (9, 70), (9, 71), (9, 72), (9, 73), (9, 74), (9, 75), (9, 76), (9, 77), (9, 78), (9, 79), (9, 80), (9, 81), (9, 82), (9, 83), (9, 84), (9, 85), (9, 86), (9, 87), (9, 88), (9, 89), (9, 90), (9, 91), (9, 92), (9, 93), (9, 94), (9, 95), (9, 96), (9, 97), (9, 98), (9, 99), (10, 100), (11, 101), (12, 102), (12, 103), (12, 104), (12, 105), (12, 106), (13, 107), (14, 108), (14, 109), (14, 110), (14, 111), (14, 112), (14, 113), (14, 114), (14, 115), (14, 116), (14, 117), (14, 118), (14, 119), (14, 120), (14, 121), (14, 122), (14, 123), (14, 124), (14, 125), (14, 126), (14, 127), (14, 128), (14, 129), (14, 130), (14, 131), (14, 132), (14, 133), (14, 134), (14, 135), (14, 136), (14, 137), (14, 138), (14, 139), (14, 140), (14, 141), (14, 142), (14, 143), (14, 144), (14, 145), (14, 146), (14, 147), (14, 148), (14, 149), (14, 150), (14, 151), (14, 152), (14, 153), (14, 154), (14, 155), (14, 156), (14, 157), (14, 158), (14, 159), (14, 160), (14, 161), (14, 162), (14, 163), (14, 164), (14, 165), (14, 166), (14, 167), (14, 168), (14, 169), (14, 170), (14, 171), (14, 172), (14, 173), (14, 174), (14, 175), (14, 176), (14, 177), (14, 178), (14, 179), (14, 180), (14, 181), (14, 182), (14, 183), (14, 184), (14, 185), (14, 186), (14, 187), (15, 188), (15, 189), (15, 190), (16, 191), (16, 192), (17, 193), (17, 194), (17, 195), (18, 196), (19, 197), (20, 198), (21, 199), (21, 200), (21, 201), (21, 202), (21, 203), (21, 204), (21, 205), (21, 206), (21, 207), (21, 208), (21, 209), (22, 210), (22, 211), (22, 212), (22, 213), (22, 214), (22, 215), (22, 216), (22, 217), (23, 218), (24, 219), (25, 220), (26, 221), (27, 222), (28, 223), (29, 224), (30, 225), (31, 226), (32, 227), (33, 228), (34, 229), (35, 230), (36, 231), (37, 232), (38, 233), (39, 234), (40, 235), (41, 236), (42, 237), (43, 238), (44, 239), (45, 240), (46, 241), (47, 242), (48, 243), (49, 244), (50, 245), (51, 246), (52, 247), (53, 248), (54, 249), (55, 250), (55, 251), (55, 252), (55, 253), (55, 254), (55, 255), (55, 256), (55, 257), (55, 258), (55, 259), (55, 260), (55, 261), (56, 262), (57, 263), (58, 264), (59, 265), (60, 266), (61, 267), (62, 268), (62, 269), (62, 270), (62, 271), (62, 272), (62, 273), (62, 274), (62, 275), (62, 276), (62, 277), (62, 278), (62, 279), (62, 280), (62, 281), (62, 282), (62, 283), (62, 284), (63, 285), (64, 286), (65, 287), (66, 288), (67, 289), (68, 290), (69, 291), (70, 292), (71, 293), (72, 294), (73, 295), (74, 296), (75, 297), (76, 298), (77, 299), (78, 300), (79, 301), (80, 302), (81, 303), (82, 304), (83, 305), (84, 306), (85, 307), (86, 308), (87, 309), (88, 310), (89, 311), (90, 312), (91, 313), (92, 314), (93, 315), (94, 316), (95, 317), (96, 318), (97, 319), (98, 320), (99, 321), (100, 322), (101, 323), (102, 324), (103, 325), (104, 326), (105, 327), (106, 328), (107, 329), (108, 330), (109, 331), (110, 332), (111, 333), (111, 334), (111, 335), (112, 336), (113, 337), (114, 338), (115, 339), (116, 340), (117, 341), (118, 342), (119, 343)]\n"
     ]
    }
   ],
   "source": [
    "from fastdtw import fastdtw\n",
    "\n",
    "framedata1 = get_framedata(\"sample1.mp4\")\n",
    "framedata2 = get_framedata(\"sample3.mp4\")\n",
    "\n",
    "dist_func = cos_sum\n",
    "distance, path = fastdtw(framedata1, framedata2, dist= dist_func)\n",
    "\n",
    "print(\"similarity:\",round(100- (distance/len(path))/90*100,3),\"%\")\n",
    "print(len(path))\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_connection = frozenset({#(0, 33),   #어떤 landmark 끼리 연결할지에 대한 정보\n",
    "           #(12, 33),\n",
    "           #(11, 33),\n",
    "           (1,2),\n",
    "           (3, 5),\n",
    "           (1, 3),\n",
    "           (2, 4),\n",
    "           (4, 6),\n",
    "           (2, 8),\n",
    "           (1, 7),\n",
    "           (7, 8),\n",
    "           (7, 9),\n",
    "           (9, 11),\n",
    "           (8, 10),\n",
    "           (10, 12),\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MediaPipe pose model\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Initialize the pose model\n",
    "pose1 = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "pose2 = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# Open the video file\n",
    "sample1 = cv2.VideoCapture(\"sample1.mp4\")\n",
    "sample2 = cv2.VideoCapture(\"sample4.mp4\")\n",
    "\n",
    "for i in range(20): #20번 째 frame 보기\n",
    "    # Read a frame from the video\n",
    "    ret1, frame1 = sample1.read()\n",
    "    ret2, frame2 = sample2.read()\n",
    "\n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "\n",
    "\n",
    "# Convert the frame from BGR to RGB\n",
    "image1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)\n",
    "image2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run the pose model on the frame\n",
    "results1 = pose1.process(image1)\n",
    "results2 = pose2.process(image2)\n",
    "\n",
    "width1 = int(sample1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height1 = int(sample1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "width2 = int(sample2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height2 = int(sample2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Draw the pose skeleton on the frame\n",
    "# a = results1.pose_landmarks.landmark\n",
    "# b = landmark_pb2.NormalizedLandmark\n",
    "# b.x= (a[11].x + a[12].x) /2\n",
    "# b.y= (a[11].y + a[12].y) /2\n",
    "# b.z= (a[11].z + a[12].z) /2\n",
    "# c = landmark_pb2.NormalizedLandmark()\n",
    "\n",
    "# results1.pose_landmarks.landmark.append(c)\n",
    "\n",
    "# a = results2.pose_landmarks.landmark\n",
    "# b = landmark_pb2.NormalizedLandmark\n",
    "# b.x= (a[11].x + a[12].x) /2\n",
    "# b.y= (a[11].y + a[12].y) /2\n",
    "# b.z= (a[11].z + a[12].z) /2\n",
    "# d = landmark_pb2.NormalizedLandmark()\n",
    "\n",
    "del results1.pose_landmarks.landmark[1:11] #delete unused landmarks\n",
    "del results2.pose_landmarks.landmark[1:11]\n",
    "\n",
    "del results1.pose_landmarks.landmark[7:13]\n",
    "del results2.pose_landmarks.landmark[7:13]\n",
    "\n",
    "del results1.pose_landmarks.landmark[13:17]\n",
    "del results2.pose_landmarks.landmark[13:17]\n",
    "\n",
    "#del results1.pose_landmarks.landmark[7:12]\n",
    "#del results2.pose_landmarks.landmark[7:12]\n",
    "\n",
    "mp_drawing.draw_landmarks(frame1, results1.pose_landmarks, pose_connection,\n",
    "                            mp_drawing.DrawingSpec(color=(255, 0, 0)),\n",
    "                            mp_drawing.DrawingSpec(color=(0, 255, 0)))\n",
    "\n",
    "mp_drawing.draw_landmarks(frame2, results2.pose_landmarks, pose_connection,\n",
    "                            mp_drawing.DrawingSpec(color=(255, 0, 0)),\n",
    "                            mp_drawing.DrawingSpec(color=(0, 255, 0)))\n",
    "\n",
    "#Resize the frame\n",
    "resized_frame1 = cv2.resize(frame1,(width1//2,height1//2))\n",
    "resized_frame2 = cv2.resize(frame2,(width2//2,height2//2))\n",
    "\n",
    "# Show the frame\n",
    "cv2.imshow(\"Video1\", resized_frame1)\n",
    "cv2.imshow(\"Video2\", resized_frame2)\n",
    "\n",
    "#Wait for the user to press 'q' to exit\n",
    "cv2.waitKey()           # 키가 입력될 때 까지 대기      \n",
    "cv2.destroyAllWindows()  # 창 모두 닫기\n",
    "\n",
    "# Release the video file and close the window\n",
    "sample1.release()\n",
    "sample2.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmark'>\n",
      "['ByteSize', 'Clear', 'ClearExtension', 'ClearField', 'CopyFrom', 'DESCRIPTOR', 'DiscardUnknownFields', 'FindInitializationErrors', 'FromString', 'HasExtension', 'HasField', 'IsInitialized', 'ListFields', 'MergeFrom', 'MergeFromString', 'PRESENCE_FIELD_NUMBER', 'ParseFromString', 'RegisterExtension', 'SerializePartialToString', 'SerializeToString', 'SetInParent', 'UnknownFields', 'VISIBILITY_FIELD_NUMBER', 'WhichOneof', 'X_FIELD_NUMBER', 'Y_FIELD_NUMBER', 'Z_FIELD_NUMBER', '_InternalParse', '_InternalSerialize', '_Modified', '_SetListener', '_UpdateOneofState', '__class__', '__deepcopy__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_cached_byte_size', '_cached_byte_size_dirty', '_decoders_by_tag', '_extensions_by_name', '_extensions_by_number', '_fields', '_is_present_in_parent', '_listener', '_listener_for_children', '_oneofs', '_unknown_field_set', '_unknown_fields', 'presence', 'visibility', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "a = landmark_pb2.NormalizedLandmark\n",
    "print(a)\n",
    "print(dir(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results1.pose_landmarks.landmark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
