{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 node를 넣으면 벡터로 변환하여 리턴하는 함수\n",
    "def makevector(pointA,pointB):\n",
    "        #calculate vector between the two points (큰 노드에서 작은 노드로)\n",
    "        vector = pointB-pointA\n",
    "        return vector\n",
    "\n",
    "# 두 vector data들의 cos 값 평균을 구하는 함수\n",
    "def cos_sum(vectordata1, vectordata2):\n",
    "    ls = [] #초기화\n",
    "    for i in range(13):\n",
    "        x = vectordata1[i]\n",
    "        y = vectordata2[i]\n",
    "        ls.append(check_degree(x,y))\n",
    "        #print(check_degree(x,y))\n",
    "        #cos = round(np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),5)\n",
    "        #ls.append( 1- (0.25*((cos+1)**2)) )  #0~1\n",
    "              \n",
    "    return sum(ls)/13\n",
    "\n",
    "#두 벡터의 각도 차\n",
    "def check_degree(i,j):\n",
    "    x = np.dot(i,j ) / (np.linalg.norm(i) * np.linalg.norm(j))\n",
    "    if x > 90:\n",
    "        x = 90\n",
    "    return math.degrees( math.acos( round(x,5) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each frame of the video \n",
    "# 영상의 각 frame별 landmark들을 추출하여 리스트에 저장 (data1,data2)\n",
    "\n",
    "def get_framedata(video_name): \n",
    "\n",
    "    # Load MediaPipe pose model\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "    \n",
    "    # Initialize the pose model\n",
    "    pose1 = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    # Open the video file\n",
    "    sample1 = cv2.VideoCapture(video_name)\n",
    "\n",
    "    framedata1 = [] #total video1 vector data by frame\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret1, frame1 = sample1.read()\n",
    "        \n",
    "        if not ret1:\n",
    "            print(\"complete vector extraction!\")\n",
    "            break\n",
    "        \n",
    "        # Convert the frame from BGR to RGB\n",
    "        frame1.flags.writeable = False\n",
    "        image1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)  \n",
    "        # Run the pose model on the frame\n",
    "        results1 = pose1.process(image1)\n",
    "        \n",
    "        if not results1.pose_landmarks:\n",
    "            continue\n",
    "            \n",
    "        landmark1 = []\n",
    "        \n",
    "        for i in range(33):\n",
    "            if i>0 and i<11: # 1,2,3,4,5,6,7,8,9,10 점 제외\n",
    "                continue\n",
    "            elif i>16 and i<23: # 17,18,19,20,21,22 점 제외\n",
    "                continue\n",
    "            elif i>28 and i<33: # 29,30,31,32 점 제외\n",
    "                continue\n",
    "            \n",
    "            marki = results1.pose_landmarks.landmark[i]\n",
    "            landmark1 += [(marki.x,marki.y,marki.z)]\n",
    "            \n",
    "        landmark1 = np.array(landmark1)\n",
    "\n",
    "        #13개의 vector를 저장하는 lists\n",
    "        vectordata1 = [] # video 1의 것\n",
    "\n",
    "        #landmark1에 들어있는 점들로 벡터 만들어서 vectordata에 저장 (13개)\n",
    "        vectordata1.append( makevector((landmark1[1] + landmark1[2])/2, landmark1[0]) )\n",
    "        vectordata1.append( makevector(landmark1[5], landmark1[3]) )\n",
    "        vectordata1.append( makevector(landmark1[3], landmark1[1]) )\n",
    "        vectordata1.append( makevector(landmark1[2], landmark1[1]) )\n",
    "        vectordata1.append( makevector(landmark1[4], landmark1[2]) )\n",
    "        vectordata1.append( makevector(landmark1[6], landmark1[4]) )\n",
    "        vectordata1.append( makevector(landmark1[8], landmark1[2]) )\n",
    "        vectordata1.append( makevector(landmark1[7], landmark1[1]) )\n",
    "        vectordata1.append( makevector(landmark1[8], landmark1[7]) )\n",
    "        vectordata1.append( makevector(landmark1[10], landmark1[8]) )\n",
    "        vectordata1.append( makevector(landmark1[12], landmark1[10]) )\n",
    "        vectordata1.append( makevector(landmark1[11], landmark1[9]) )\n",
    "        vectordata1.append( makevector(landmark1[9], landmark1[7]) )\n",
    "        \n",
    "        framedata1+= [vectordata1]\n",
    "\n",
    "        # Draw the pose skeleton on the frame\n",
    "        # mp_drawing.draw_landmarks(frame1, results1.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "        #                           mp_drawing.DrawingSpec(color=(255, 0, 0)),\n",
    "        #                          mp_drawing.DrawingSpec(color=(0, 255, 0)))\n",
    "        \n",
    "        # mp_drawing.draw_landmarks(frame2, results2.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "        #                           mp_drawing.DrawingSpec(color=(255, 0, 0)),\n",
    "        #                           mp_drawing.DrawingSpec(color=(0, 255, 0)))\n",
    "        # Resize the frame\n",
    "        # resized_frame1 = cv2.resize(frame1,(width1//4,height1//4))\n",
    "        # resized_frame2 = cv2.resize(frame2,(width2//4,height2//4))\n",
    "\n",
    "        # Show the frame\n",
    "        # cv2.imshow(\"Video1\", resized_frame1)\n",
    "        # cv2.imshow(\"Video2\", resized_frame2)\n",
    "        \n",
    "        # Wait for the user to press 'q' to exit\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "        \n",
    "    # Release the video file and close the window\n",
    "    sample1.release()\n",
    "    return framedata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete vector extraction!\n",
      "complete vector extraction!\n",
      "similarity: 71.154 %\n",
      "136\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 15), (16, 16), (17, 17), (18, 18), (19, 19), (20, 20), (21, 21), (22, 22), (23, 23), (24, 24), (25, 25), (26, 26), (27, 27), (28, 28), (29, 29), (30, 30), (31, 31), (32, 32), (33, 33), (34, 34), (35, 35), (36, 36), (37, 37), (38, 38), (39, 39), (40, 40), (41, 41), (42, 42), (43, 42), (44, 43), (45, 44), (46, 45), (47, 46), (48, 47), (49, 48), (50, 49), (51, 50), (52, 51), (53, 52), (54, 53), (55, 54), (56, 55), (57, 56), (58, 57), (59, 58), (60, 59), (61, 60), (62, 61), (63, 62), (64, 63), (65, 64), (66, 65), (67, 66), (68, 67), (69, 68), (70, 69), (71, 70), (72, 71), (73, 72), (74, 73), (75, 74), (76, 75), (77, 76), (78, 77), (79, 78), (80, 79), (81, 80), (82, 81), (83, 82), (84, 83), (84, 84), (84, 85), (84, 86), (85, 87), (86, 88), (87, 89), (88, 90), (89, 91), (90, 92), (91, 93), (92, 94), (93, 95), (93, 96), (93, 97), (93, 98), (93, 99), (94, 100), (94, 101), (94, 102), (94, 103), (94, 104), (94, 105), (94, 106), (94, 107), (94, 108), (94, 109), (95, 110), (96, 111), (97, 112), (98, 112), (99, 112), (100, 112), (101, 112), (102, 112), (103, 113), (104, 114), (105, 115), (106, 116), (107, 117), (108, 118), (109, 119), (110, 120), (111, 120), (112, 120), (113, 120), (114, 120), (115, 120), (116, 120), (117, 120), (118, 120), (119, 120)]\n"
     ]
    }
   ],
   "source": [
    "from fastdtw import fastdtw\n",
    "\n",
    "framedata1 = get_framedata(\"sample1.mp4\")\n",
    "framedata2 = get_framedata(\"sample4.mp4\")\n",
    "\n",
    "distance, path = fastdtw(framedata1, framedata2, dist=cos_sum)\n",
    "\n",
    "print(\"similarity:\",round(100- (distance/len(path))/90*100,3),\"%\")\n",
    "print(len(path))\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_connection = frozenset({#(0, 33),   #어떤 landmark를 연결할지에 대한 정보\n",
    "           #(12, 33),\n",
    "           #(11, 33),\n",
    "           (1,2),\n",
    "           (3, 5),\n",
    "           (1, 3),\n",
    "           (2, 4),\n",
    "           (4, 6),\n",
    "           (2, 8),\n",
    "           (1, 7),\n",
    "           (7, 8),\n",
    "           (7, 9),\n",
    "           (9, 11),\n",
    "           (8, 10),\n",
    "           (10, 12),\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MediaPipe pose model\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Initialize the pose model\n",
    "pose1 = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "pose2 = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# Open the video file\n",
    "sample1 = cv2.VideoCapture(\"sample1.mp4\")\n",
    "sample2 = cv2.VideoCapture(\"sample4.mp4\")\n",
    "\n",
    "for i in range(20): #20번 째 frame 보기\n",
    "    # Read a frame from the video\n",
    "    ret1, frame1 = sample1.read()\n",
    "    ret2, frame2 = sample2.read()\n",
    "\n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "\n",
    "\n",
    "# Convert the frame from BGR to RGB\n",
    "image1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)\n",
    "image2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run the pose model on the frame\n",
    "results1 = pose1.process(image1)\n",
    "results2 = pose2.process(image2)\n",
    "\n",
    "width1 = int(sample1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height1 = int(sample1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "width2 = int(sample2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height2 = int(sample2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Draw the pose skeleton on the frame\n",
    "# a = results1.pose_landmarks.landmark\n",
    "# b = landmark_pb2.NormalizedLandmark\n",
    "# b.x= (a[11].x + a[12].x) /2\n",
    "# b.y= (a[11].y + a[12].y) /2\n",
    "# b.z= (a[11].z + a[12].z) /2\n",
    "# c = landmark_pb2.NormalizedLandmark()\n",
    "\n",
    "# results1.pose_landmarks.landmark.append(c)\n",
    "\n",
    "# a = results2.pose_landmarks.landmark\n",
    "# b = landmark_pb2.NormalizedLandmark\n",
    "# b.x= (a[11].x + a[12].x) /2\n",
    "# b.y= (a[11].y + a[12].y) /2\n",
    "# b.z= (a[11].z + a[12].z) /2\n",
    "# d = landmark_pb2.NormalizedLandmark()\n",
    "\n",
    "del results1.pose_landmarks.landmark[1:11]\n",
    "del results2.pose_landmarks.landmark[1:11]\n",
    "\n",
    "del results1.pose_landmarks.landmark[7:13]\n",
    "del results2.pose_landmarks.landmark[7:13]\n",
    "\n",
    "del results1.pose_landmarks.landmark[14:18]\n",
    "del results2.pose_landmarks.landmark[14:18]\n",
    "\n",
    "#del results1.pose_landmarks.landmark[7:12]\n",
    "#del results2.pose_landmarks.landmark[7:12]\n",
    "\n",
    "mp_drawing.draw_landmarks(frame1, results1.pose_landmarks, pose_connection,\n",
    "                            mp_drawing.DrawingSpec(color=(255, 0, 0)),\n",
    "                            mp_drawing.DrawingSpec(color=(0, 255, 0)))\n",
    "\n",
    "mp_drawing.draw_landmarks(frame2, results2.pose_landmarks, pose_connection,\n",
    "                            mp_drawing.DrawingSpec(color=(255, 0, 0)),\n",
    "                            mp_drawing.DrawingSpec(color=(0, 255, 0)))\n",
    "\n",
    "#Resize the frame\n",
    "resized_frame1 = cv2.resize(frame1,(width1//2,height1//2))\n",
    "resized_frame2 = cv2.resize(frame2,(width2//2,height2//2))\n",
    "\n",
    "# Show the frame\n",
    "cv2.imshow(\"Video1\", resized_frame1)\n",
    "cv2.imshow(\"Video2\", resized_frame2)\n",
    "\n",
    "#Wait for the user to press 'q' to exit\n",
    "cv2.waitKey()           # 키가 입력될 때 까지 대기      \n",
    "cv2.destroyAllWindows()  # 창 모두 닫기\n",
    "\n",
    "# Release the video file and close the window\n",
    "sample1.release()\n",
    "sample2.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.framework.formats.landmark_pb2.NormalizedLandmark'>\n",
      "['ByteSize', 'Clear', 'ClearExtension', 'ClearField', 'CopyFrom', 'DESCRIPTOR', 'DiscardUnknownFields', 'FindInitializationErrors', 'FromString', 'HasExtension', 'HasField', 'IsInitialized', 'ListFields', 'MergeFrom', 'MergeFromString', 'PRESENCE_FIELD_NUMBER', 'ParseFromString', 'RegisterExtension', 'SerializePartialToString', 'SerializeToString', 'SetInParent', 'UnknownFields', 'VISIBILITY_FIELD_NUMBER', 'WhichOneof', 'X_FIELD_NUMBER', 'Y_FIELD_NUMBER', 'Z_FIELD_NUMBER', '_InternalParse', '_InternalSerialize', '_Modified', '_SetListener', '_UpdateOneofState', '__class__', '__deepcopy__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_cached_byte_size', '_cached_byte_size_dirty', '_decoders_by_tag', '_extensions_by_name', '_extensions_by_number', '_fields', '_is_present_in_parent', '_listener', '_listener_for_children', '_oneofs', '_unknown_field_set', '_unknown_fields', 'presence', 'visibility', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "a = landmark_pb2.NormalizedLandmark\n",
    "print(a)\n",
    "print(dir(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results1.pose_landmarks.landmark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
